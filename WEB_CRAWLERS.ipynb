{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gI3vqulJrJt1"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from urllib.parse import urljoin, urlparse, urlsplit\n",
        "import time\n",
        "from urllib.robotparser import RobotFileParser\n",
        "# visits up to 100 pages within the MIT domain,\n",
        "# adhering to robots.txt rules,\n",
        "#logs the canonical URLs of visited pages and their outgoing links to a text file, while pausing for 5 seconds between page visits."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 1  #visit webpages in tht domain\n",
        "start_url = 'http://www.mit.edu'\n",
        "urls_to_visit = set([start_url])\n",
        "visited_urls = set()\n",
        "domain_pattern = re.compile('^https?://(www\\.)?mit\\.edu')# filter out URLs from other domains.\n",
        "html_pages_visited = 0\n",
        "output_file = open('link.txt', 'w')\n",
        "#Task 8\n",
        "rp = RobotFileParser()# to communicate with web crawlers or robots\n",
        "rp.set_url('https://www.mit.edu/robots.txt')\n",
        "rp.read()\n",
        "#Task 2\n",
        "while urls_to_visit and html_pages_visited < 100:#till 100 webpages\n",
        "    url = urls_to_visit.pop()\n",
        "    if url in visited_urls:\n",
        "        continue\n",
        "    # Task 8\n",
        "    if not rp.can_fetch('*', url):\n",
        "        continue\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "    except requests.exceptions.RequestException:\n",
        "        continue\n",
        "    content_type = response.headers.get('Content-Type')\n",
        "    if content_type is not None:\n",
        "        #Task 3:Detect whether a page you visit is an HTML doc or not?\n",
        "        if 'text/html' in content_type:#whether html or not\n",
        "            visited_urls.add(url)\n",
        "            html_pages_visited += 1\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')#asy parsing and navigation of the HTML\n",
        "            links = soup.find_all('a')#returns a list of tag objects representing the links present on the page.\n",
        "            #Task 4: Detecting all outgoing links on each HTML page\n",
        "            for link in links:\n",
        "                href = link.get('href')#href variable will contain the URL or link destination.\n",
        "                if href is not None:\n",
        "                  #join base URL (url) with the relative URL extracted from the href\n",
        "                    #Task 4 p2: This will require you to convert some links to a canonical form\n",
        "                    href = urljoin(url, href.split('#')[0])#  remove  fragment identifier#section\n",
        "                    parsed_href = urlparse(href)\n",
        "                    if parsed_href.netloc == 'www.mit.edu' or parsed_href.netloc == 'mit.edu':#only links within the mit.edu domain are processed.\n",
        "                        #Task 4: Follow only links to pages you have not yet visited.\n",
        "                        if href not in visited_urls:\n",
        "                            urls_to_visit.add(href)\n",
        "                        #Task 5: For every web page you visit, write its canonical URL and the canonical URLs of each outgoing link to an HTML separated by a single space, to a plain text output file, with one line per visited page.\n",
        "                        output_file.write(f'{urljoin(url, urlsplit(url).path)} {href}\\n')\n",
        "            #Task 5: For every web page you visit, write its canonical URL and the canonical URLs of each outgoing link to an HTML separated by a single space, to a plain text output file, with one line per visited page.\n",
        "            output_file.write(f'{urljoin(url, urlsplit(url).path)}\\n')\n",
        "        #Task 3:Detect whether a page you visit is a PDF using the appropriate Content-Type HTTP header.\n",
        "        elif 'application/pdf' in content_type:\n",
        "            #Task 5: For every web page you visit, write its canonical URL and the canonical URLs of each outgoing link to PDF file separated by a single space, to a plain text output file, with one line per visited page.\n",
        "            output_file.write(f'{urljoin(url, urlsplit(url).path)}\\n')\n",
        "    #Task 3:Detect whether a page you visit is something else using the appropriate Content-Type HTTP header.\n",
        "    else:\n",
        "        #Task 5: For every web page you visit, write its canonical URL and the canonical URLs of each outgoing link separated by a single space, to a plain text output file, with one line per visited page.\n",
        "        output_file.write(f'{urljoin(url, urlsplit(url).path)}\\n')\n",
        "    visited_urls.add(url)\n",
        "    #Task 8: Your crawler must visit at most one page per five seconds.\n",
        "    time.sleep(5)\n",
        "output_file.close()"
      ],
      "metadata": {
        "id": "ZoyTRemHEv3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7C-pryPr2AxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 6\n",
        "  #Put some thought into the pages you decide to visit, and the order in which you visit them.\n",
        "    #When i was deciding which pages should i vissit and it what oorder we prioritize the following:\n",
        "      #1. we should give respect for the web server's resources\n",
        "      #2. Relevance of the pages\n",
        "      #3. Quality of the pages\n",
        "  #Is there any way links on web pages could cause your crawler to misbehave?\n",
        "      # Large pages\n",
        "      # if we have infinite loops\n",
        "      # if there are Dynamic pages\n",
        "      # if there are Malicious links"
      ],
      "metadata": {
        "id": "r9lGrBSSizvw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6CK94Lqp2BcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J3X0AVsm2B4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DJkmY_SN2CS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3zRTIFo32CtZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}